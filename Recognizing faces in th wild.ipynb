{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88M3bdRHZPOm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "jH3HJ6BEZW9H",
    "outputId": "d911c6e0-b2ce-4064-9f79-d0274bac8862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvKfqDQWZbzE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "HqKKrcBEH8at",
    "outputId": "aa46abde-fc2d-4920-c307-acd4161204b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
      "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-9spayvfa\n",
      "  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-9spayvfa\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.6) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.6) (1.3.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.6) (2.8.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.6) (4.3.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.6) (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.6) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.6) (3.13)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->keras-vggface==0.6) (0.46)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.6) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.6) (1.1.0)\n",
      "Building wheels for collected packages: keras-vggface\n",
      "  Building wheel for keras-vggface (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-vggface: filename=keras_vggface-0.6-cp36-none-any.whl size=8311 sha256=153702d8b32665440849dfff0c243120664fa9f127753787a8f1236190db5e6b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fup5o0ta/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\n",
      "Successfully built keras-vggface\n",
      "Installing collected packages: keras-vggface\n",
      "Successfully installed keras-vggface-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/rcmalli/keras-vggface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baw94PTzGtFu"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqk0RV7bG7Y7"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice,sample\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4sp3CJKFHRWy",
    "outputId": "307bb811-4383-4cda-c69a-36efbd4c1f65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.layers import Input,Dense,GlobalMaxPool2D,GlobalAvgPool2D,Concatenate,Multiply,Dropout,Subtract,Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras import backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MK8qb18aHW30"
   },
   "outputs": [],
   "source": [
    "train_file_path='/content/drive/My Drive/Recognizing_Faces_in_the_Wild/train_relationships.csv'\n",
    "train_folders_path='/content/drive/My Drive/Recognizing_Faces_in_the_Wild/train/'\n",
    "val_families='F09' #families which has F09*** in folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eWD1bVlfHb-I",
    "outputId": "6e5d69ff-c041-4e25-eecd-097ae5ea508a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 368 ms, sys: 263 ms, total: 631 ms\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_images=glob(train_folders_path+'*/*/*.jpg') #paths of all images\n",
    "train_images=[x for x in all_images if val_families not in x] #path of images used for training\n",
    "val_images=[x for x in all_images if val_families in x] #path of validation images (belonging to families starting with F09***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVC3V2_nHeJm"
   },
   "outputs": [],
   "source": [
    "ppl=[x.split('/')[-3]+'/'+x.split(\"/\")[-2] for x in all_images] #obtaining the people in the format give in train_relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pJigNvQHgWd"
   },
   "outputs": [],
   "source": [
    "#Mapping people to their faces (list of faces)\n",
    "train_person_to_images_map=defaultdict(list)\n",
    "for x in train_images:\n",
    "  train_person_to_images_map[x.split('/')[-3]+'/'+x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "val_person_to_images_map=defaultdict(list)\n",
    "for x in val_images:\n",
    "  val_person_to_images_map[x.split('/')[-3]+'/'+x.split('/')[-2]].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYgz2Fc1HijF"
   },
   "outputs": [],
   "source": [
    "#Obtaining relationship pairs and converting them to tuples\n",
    "relationships = pd.read_csv(train_file_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
    "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MRB2SzPHkQS"
   },
   "outputs": [],
   "source": [
    "#Diving the tuples into train and validation\n",
    "train=[x for x in relationships if val_families not in x[0]]\n",
    "val=[x for x in relationships if val_families in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HposeONAHmMG"
   },
   "outputs": [],
   "source": [
    "#reads the image and converts into numpy aarray and finally returns the image processed as required by VGGFace\n",
    "def img2arr(path):\n",
    "  img=cv2.imread(path)\n",
    "  img=np.array(img).astype(np.float)\n",
    "  return preprocess_input(img,version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVbInAUNHokD"
   },
   "outputs": [],
   "source": [
    "#Generator to use with fit_generator to generate data in batches\n",
    "def data_generator(list_tuples,person_to_images_map,batch_size=16):\n",
    "  ppl=list(person_to_images_map.keys())\n",
    "  while True:\n",
    "    batch_tuples=sample(list_tuples,batch_size//2)\n",
    "    labels=[1]*len(batch_tuples)\n",
    "    while len(batch_tuples)<batch_size:\n",
    "      p1=choice(ppl)\n",
    "      p2=choice(ppl)\n",
    "\n",
    "      if p1!=p2 and (p1,p2) not in list_tuples and (p2,p1) not in list_tuples:\n",
    "        batch_tuples.append((p1,p2))\n",
    "        labels.append(0)\n",
    "    for x in batch_tuples:\n",
    "      if not len(person_to_images_map[x[0]]):\n",
    "        print(x[0])\n",
    "    X1=[choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "    X1=np.array([img2arr(x) for x in X1])\n",
    "    X2=[choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "    X2=np.array([img2arr(x) for x in X2])\n",
    "\n",
    "    yield [X1,X2],labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Model Architecture</h4>\n",
    "<img src='model_architecture.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9oDzWZbHrUB"
   },
   "outputs": [],
   "source": [
    "#Model architecture\n",
    "def build_model():\n",
    "  input1=Input(shape=(224,224,3))\n",
    "  input2=Input(shape=(224,224,3))\n",
    "\n",
    "  base_model=VGGFace(model='resnet50',include_top=False)\n",
    "\n",
    "  '''for x in base_model.layers[:-3]:\n",
    "    x.trainable=True'''\n",
    "  \n",
    "  x1=base_model(input1)\n",
    "  x2=base_model(input2)\n",
    "\n",
    "  x1=Concatenate(axis=-1)([GlobalMaxPool2D()(x1),GlobalAvgPool2D()(x1)])\n",
    "  x2=Concatenate(axis=-1)([GlobalMaxPool2D()(x2),GlobalAvgPool2D()(x2)])\n",
    "\n",
    "  x3=Subtract()([x1,x2])\n",
    "  x3=Multiply()([x3,x3])\n",
    "  #x=Multiply()([x3,x3])\n",
    "  \n",
    "  #x=Multiply()([x1,x2])\n",
    "  x1_=Multiply()([x1,x1])\n",
    "  x2_=Multiply()([x2,x2])\n",
    "  x4=Subtract()([x1_,x2_])\n",
    "\n",
    "  x5=Multiply()([x1,x2])\n",
    "\n",
    "  x=Concatenate(axis=-1)([x3,x4,x5])\n",
    "\n",
    "  x=Dense(100,activation='relu')(x)\n",
    "  x=Dropout(0.01)(x)\n",
    "  out=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "  model=Model([input1,input2],out)\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',metrics=['acc'],optimizer=Adam(0.00001))\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hnboKkZHtWI"
   },
   "outputs": [],
   "source": [
    "file_path='/content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRISdKJEHwIQ"
   },
   "outputs": [],
   "source": [
    "#Using callbacks\n",
    "checkpoint=ModelCheckpoint(file_path,monitor='val_acc',verbose=1,save_best_only=True,mode='max') #Saves the best model based on val_acc\n",
    "\n",
    "reduce_lr_on_plateau=ReduceLROnPlateau(monitor='val_acc',mode='max',factor=0.1,patience=20,verbose=1) #Reduces the learning rate when val_acc is not improving\n",
    "\n",
    "callbacks_list=[checkpoint,reduce_lr_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WPyqxYFlHzEO",
    "outputId": "96664e21-a2f3-4914-c172-0c038ec6ea61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 16:28:16.535361 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0824 16:28:16.583397 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0824 16:28:16.593642 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0824 16:28:16.634713 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0824 16:28:16.636055 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0824 16:28:19.507476 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0824 16:28:19.680394 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0824 16:28:24.887172 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94699520/94694792 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 16:28:37.291864 140109749909376 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0824 16:28:37.336609 140109749909376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0824 16:28:37.348724 140109749909376 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_resnet50 (Model)        multiple             23561152    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 2048)         0           vggface_resnet50[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           vggface_resnet50[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 2048)         0           vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 2048)         0           vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4096)         0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4096)         0           global_max_pooling2d_2[0][0]     \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 4096)         0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 4096)         0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 4096)         0           concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 4096)         0           subtract_1[0][0]                 \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 4096)         0           multiply_2[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8192)         0           multiply_1[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          819300      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,380,553\n",
      "Trainable params: 24,327,433\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 4.5966 - acc: 0.5784 - val_loss: 4.6615 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60625, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 222s 1s/step - loss: 3.7302 - acc: 0.6100 - val_loss: 4.3003 - val_acc: 0.5900\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.60625\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 2.5468 - acc: 0.6428 - val_loss: 2.9927 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.60625\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 1.3980 - acc: 0.6447 - val_loss: 1.6171 - val_acc: 0.6075\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.60625 to 0.60750, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.8909 - acc: 0.6466 - val_loss: 1.0744 - val_acc: 0.6012\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.60750\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.6753 - acc: 0.6856 - val_loss: 0.8883 - val_acc: 0.6369\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.60750 to 0.63687, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.6183 - acc: 0.6934 - val_loss: 0.7925 - val_acc: 0.6394\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.63687 to 0.63938, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.5872 - acc: 0.7028 - val_loss: 0.6485 - val_acc: 0.6700\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.63938 to 0.67000, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.5466 - acc: 0.7250 - val_loss: 0.6808 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.67000\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.5409 - acc: 0.7188 - val_loss: 0.6705 - val_acc: 0.6756\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.67000 to 0.67563, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 221s 1s/step - loss: 0.5123 - acc: 0.7522 - val_loss: 0.6250 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.67563 to 0.68437, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.5045 - acc: 0.7481 - val_loss: 0.5887 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.68437 to 0.70937, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4758 - acc: 0.7653 - val_loss: 0.6255 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.70937 to 0.71125, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4876 - acc: 0.7609 - val_loss: 0.6244 - val_acc: 0.7013\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.71125\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4698 - acc: 0.7750 - val_loss: 0.5819 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.71125\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4597 - acc: 0.7694 - val_loss: 0.5655 - val_acc: 0.7181\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.71125 to 0.71813, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4473 - acc: 0.7937 - val_loss: 0.6061 - val_acc: 0.6994\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.71813\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.4580 - acc: 0.7766 - val_loss: 0.5775 - val_acc: 0.7231\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.71813 to 0.72313, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4487 - acc: 0.7887 - val_loss: 0.5711 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.72313\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4194 - acc: 0.8084 - val_loss: 0.5469 - val_acc: 0.7231\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.72313\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4380 - acc: 0.7947 - val_loss: 0.5562 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.72313 to 0.73562, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4135 - acc: 0.8034 - val_loss: 0.6094 - val_acc: 0.7163\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.73562\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4111 - acc: 0.8072 - val_loss: 0.6013 - val_acc: 0.7338\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.73562\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.4193 - acc: 0.8050 - val_loss: 0.5393 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.73562 to 0.74062, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3886 - acc: 0.8247 - val_loss: 0.5326 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.74062 to 0.75500, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3901 - acc: 0.8262 - val_loss: 0.5311 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.75500\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3995 - acc: 0.8137 - val_loss: 0.5625 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.75500\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.3856 - acc: 0.8228 - val_loss: 0.5412 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.75500\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.4082 - acc: 0.8131 - val_loss: 0.5332 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.75500\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.3997 - acc: 0.8166 - val_loss: 0.5402 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.75500\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3807 - acc: 0.8269 - val_loss: 0.5283 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.75500\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3582 - acc: 0.8409 - val_loss: 0.5608 - val_acc: 0.7556\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.75500 to 0.75562, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3672 - acc: 0.8344 - val_loss: 0.5541 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.75562\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3618 - acc: 0.8359 - val_loss: 0.5389 - val_acc: 0.7431\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.75562\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3714 - acc: 0.8350 - val_loss: 0.5081 - val_acc: 0.7481\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.75562\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.3617 - acc: 0.8363 - val_loss: 0.5346 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.75562\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.3537 - acc: 0.8466 - val_loss: 0.5530 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.75562 to 0.76250, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3423 - acc: 0.8469 - val_loss: 0.5180 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76250\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3372 - acc: 0.8491 - val_loss: 0.5432 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76250\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.3421 - acc: 0.8403 - val_loss: 0.5292 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76250\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.3350 - acc: 0.8519 - val_loss: 0.5316 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.76250 to 0.76562, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3281 - acc: 0.8562 - val_loss: 0.5189 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76562\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3217 - acc: 0.8562 - val_loss: 0.5104 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.76562 to 0.76875, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3195 - acc: 0.8619 - val_loss: 0.5289 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76875\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3217 - acc: 0.8594 - val_loss: 0.5167 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76875\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.3085 - acc: 0.8659 - val_loss: 0.5231 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.76875 to 0.77000, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.3188 - acc: 0.8566 - val_loss: 0.5481 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.77000\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3026 - acc: 0.8656 - val_loss: 0.5708 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.77000\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3080 - acc: 0.8653 - val_loss: 0.5908 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.77000\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.3016 - acc: 0.8731 - val_loss: 0.6103 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.77000\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2951 - acc: 0.8788 - val_loss: 0.5428 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.77000\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.2950 - acc: 0.8672 - val_loss: 0.5885 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.77000\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2858 - acc: 0.8831 - val_loss: 0.5648 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.77000\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.2924 - acc: 0.8738 - val_loss: 0.5572 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.77000\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2812 - acc: 0.8778 - val_loss: 0.5494 - val_acc: 0.7350\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.77000\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2761 - acc: 0.8847 - val_loss: 0.5654 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.77000\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.2954 - acc: 0.8769 - val_loss: 0.5998 - val_acc: 0.7212\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.77000\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2730 - acc: 0.8897 - val_loss: 0.5281 - val_acc: 0.7631\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.77000\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.2781 - acc: 0.8834 - val_loss: 0.5371 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.77000\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 215s 1s/step - loss: 0.2741 - acc: 0.8781 - val_loss: 0.5203 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.77000 to 0.77125, saving model to /content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vgg_face.h5\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 215s 1s/step - loss: 0.2742 - acc: 0.8769 - val_loss: 0.5291 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.77125\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 215s 1s/step - loss: 0.2416 - acc: 0.8997 - val_loss: 0.5408 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.77125\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.2606 - acc: 0.8887 - val_loss: 0.5836 - val_acc: 0.7519\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.77125\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2487 - acc: 0.8947 - val_loss: 0.5691 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.77125\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2779 - acc: 0.8822 - val_loss: 0.6135 - val_acc: 0.7556\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.77125\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.2517 - acc: 0.8884 - val_loss: 0.6338 - val_acc: 0.7225\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.77125\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.2510 - acc: 0.8947 - val_loss: 0.6476 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.77125\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2547 - acc: 0.8922 - val_loss: 0.5922 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.77125\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2627 - acc: 0.8922 - val_loss: 0.5574 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.77125\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 219s 1s/step - loss: 0.2556 - acc: 0.8928 - val_loss: 0.5817 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.77125\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2352 - acc: 0.9056 - val_loss: 0.5882 - val_acc: 0.7494\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.77125\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 219s 1s/step - loss: 0.2346 - acc: 0.9072 - val_loss: 0.5722 - val_acc: 0.7519\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.77125\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 221s 1s/step - loss: 0.2307 - acc: 0.9047 - val_loss: 0.5837 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.77125\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 221s 1s/step - loss: 0.2233 - acc: 0.9156 - val_loss: 0.6744 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.77125\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 221s 1s/step - loss: 0.2204 - acc: 0.9131 - val_loss: 0.6139 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.77125\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2286 - acc: 0.9038 - val_loss: 0.5780 - val_acc: 0.7669\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.77125\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2263 - acc: 0.9091 - val_loss: 0.6923 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.77125\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2358 - acc: 0.9050 - val_loss: 0.5899 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.77125\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2392 - acc: 0.9084 - val_loss: 0.5544 - val_acc: 0.7588\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.77125\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 220s 1s/step - loss: 0.2224 - acc: 0.9084 - val_loss: 0.6809 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.77125\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 219s 1s/step - loss: 0.2132 - acc: 0.9134 - val_loss: 0.6425 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.77125\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.2076 - acc: 0.9203 - val_loss: 0.6043 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.77125\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 219s 1s/step - loss: 0.1940 - acc: 0.9266 - val_loss: 0.6390 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.77125\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.2066 - acc: 0.9191 - val_loss: 0.6349 - val_acc: 0.7369\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.77125\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.1928 - acc: 0.9288 - val_loss: 0.6372 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.77125\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.1713 - acc: 0.9341 - val_loss: 0.6413 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.77125\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.1877 - acc: 0.9269 - val_loss: 0.6674 - val_acc: 0.7294\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.77125\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.1787 - acc: 0.9341 - val_loss: 0.6138 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.77125\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.1866 - acc: 0.9300 - val_loss: 0.6636 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.77125\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.1723 - acc: 0.9378 - val_loss: 0.6543 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.77125\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 218s 1s/step - loss: 0.1800 - acc: 0.9313 - val_loss: 0.6855 - val_acc: 0.7300\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.77125\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1707 - acc: 0.9322 - val_loss: 0.7071 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.77125\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1698 - acc: 0.9378 - val_loss: 0.7594 - val_acc: 0.7163\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.77125\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1859 - acc: 0.9250 - val_loss: 0.7116 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.77125\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1700 - acc: 0.9350 - val_loss: 0.5837 - val_acc: 0.7588\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.77125\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1718 - acc: 0.9366 - val_loss: 0.6641 - val_acc: 0.7388\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.77125\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1795 - acc: 0.9313 - val_loss: 0.6765 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.77125\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 216s 1s/step - loss: 0.1682 - acc: 0.9359 - val_loss: 0.6770 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.77125\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1689 - acc: 0.9363 - val_loss: 0.7014 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.77125\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 217s 1s/step - loss: 0.1632 - acc: 0.9384 - val_loss: 0.7112 - val_acc: 0.7256\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.77125\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    }
   ],
   "source": [
    "model=build_model()\n",
    "\n",
    "history=model.fit_generator(data_generator(train,train_person_to_images_map,batch_size=16),\\\n",
    "                    use_multiprocessing=True,\\\n",
    "                    validation_data=data_generator(val, val_person_to_images_map, batch_size=16),\\\n",
    "                    epochs=100,verbose=1,workers=4,callbacks=callbacks_list,steps_per_epoch=200,validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TpUoo_ibLj4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJeSrWrrk0fs"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/history.pkl','wb') as f:\n",
    "  pickle.dump(history,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATechU_xm6oi"
   },
   "outputs": [],
   "source": [
    "#saving the model\n",
    "model.save('/content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vggface.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXC9j6AIAeTF"
   },
   "outputs": [],
   "source": [
    "model=load_model('/content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/vggface.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZsZQK5j04w-"
   },
   "outputs": [],
   "source": [
    "test_path='/content/drive/My Drive/Recognizing_Faces_in_the_Wild/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OkOwPTPMxv0Y"
   },
   "outputs": [],
   "source": [
    "submission=pd.read_csv('/content/drive/My Drive/Recognizing_Faces_in_the_Wild/sample_submission.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mkN6n7Qkx_fV",
    "outputId": "fdb9d159-81d4-418f-a44e-c5b4f4afb3ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_pair</th>\n",
       "      <th>is_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>face05508.jpg-face01210.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face05750.jpg-face00898.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>face05820.jpg-face03938.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>face02104.jpg-face01172.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>face02428.jpg-face05611.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      img_pair  is_related\n",
       "0  face05508.jpg-face01210.jpg           0\n",
       "1  face05750.jpg-face00898.jpg           0\n",
       "2  face05820.jpg-face03938.jpg           0\n",
       "3  face02104.jpg-face01172.jpg           0\n",
       "4  face02428.jpg-face05611.jpg           0"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhDKk7rb2rmx"
   },
   "outputs": [],
   "source": [
    "#generates test data in batches\n",
    "def test_batch(test_pairs,size=32):\n",
    "  return (test_pairs[pos:pos+size] for pos in range(0,len(test_pairs),size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FEfXxAcMx5EU",
    "outputId": "4bef1760-1894-452b-8a89-574e24a5cdb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [33:40, 10.38s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "for batch in tqdm(test_batch(submission.img_pair.values)):\n",
    "    X1 = [x.split(\"-\")[0] for x in batch]\n",
    "    X1 = np.array([img2arr(test_path + x) for x in X1])\n",
    "\n",
    "    X2 = [x.split(\"-\")[1] for x in batch]\n",
    "    X2 = np.array([img2arr(test_path + x) for x in X2])\n",
    "\n",
    "    pred = model.predict([X1, X2]).ravel().tolist()\n",
    "    predictions += pred\n",
    "\n",
    "submission['is_related'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LJV4xjwNmrU"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"/content/drive/My Drive/Recognizing_Faces_in_the_Wild/20190824/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note</strong>\n",
    "\n",
    "Used features (x1-x2)^2, (x1^2 - x2^2) and (x1*x2) \n",
    "\n",
    "Used different validation sets while training five models and took the average of predictions of all models to improve the score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20190824.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
